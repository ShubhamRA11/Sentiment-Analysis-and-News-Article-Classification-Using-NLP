{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6a2693",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ad6c1",
   "metadata": {},
   "source": [
    "## Part B : News Article Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f40a68",
   "metadata": {},
   "source": [
    "### Task 1: Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfc668",
   "metadata": {},
   "source": [
    "####  Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c7654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe992a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_excel(\"data_news.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a66fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.columns = df_news.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2417a140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['category', 'headline', 'links', 'short_description', 'keywords']\n"
     ]
    }
   ],
   "source": [
    "print(df_news.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "263d3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   category                                           headline  \\\n",
      "0  WELLNESS              143 Miles in 35 Days: Lessons Learned   \n",
      "1  WELLNESS       Talking to Yourself: Crazy or Crazy Helpful?   \n",
      "2  WELLNESS  Crenezumab: Trial Will Gauge Whether Alzheimer...   \n",
      "3  WELLNESS                     Oh, What a Difference She Made   \n",
      "4  WELLNESS                                   Green Superfoods   \n",
      "\n",
      "                                               links  \\\n",
      "0  https://www.huffingtonpost.com/entry/running-l...   \n",
      "1  https://www.huffingtonpost.com/entry/talking-t...   \n",
      "2  https://www.huffingtonpost.com/entry/crenezuma...   \n",
      "3  https://www.huffingtonpost.com/entry/meaningfu...   \n",
      "4  https://www.huffingtonpost.com/entry/green-sup...   \n",
      "\n",
      "                                   short_description  \\\n",
      "0  Resting is part of training. I've confirmed wh...   \n",
      "1  Think of talking to yourself as a tool to coac...   \n",
      "2  The clock is ticking for the United States to ...   \n",
      "3  If you want to be busy, keep trying to be perf...   \n",
      "4  First, the bad news: Soda bread, corned beef a...   \n",
      "\n",
      "                             keywords  \n",
      "0                     running-lessons  \n",
      "1           talking-to-yourself-crazy  \n",
      "2  crenezumab-alzheimers-disease-drug  \n",
      "3                     meaningful-life  \n",
      "4                    green-superfoods  \n"
     ]
    }
   ],
   "source": [
    "print(df_news.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c67b92cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category                0\n",
      "headline                0\n",
      "links                   0\n",
      "short_description       6\n",
      "keywords             2706\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_news.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f1671f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELLNESS          5000\n",
      "POLITICS          5000\n",
      "ENTERTAINMENT     5000\n",
      "TRAVEL            5000\n",
      "STYLE & BEAUTY    5000\n",
      "PARENTING         5000\n",
      "FOOD & DRINK      5000\n",
      "WORLD NEWS        5000\n",
      "BUSINESS          5000\n",
      "SPORTS            5000\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_news['category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce478a",
   "metadata": {},
   "source": [
    "#### Step 2: Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64510771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "      <td>resting part training ive confirmed sort alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "      <td>think talking tool coach challenge narrate exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "      <td>clock ticking united state find cure team work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "      <td>want busy keep trying perfect want happy focus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "      <td>first bad news soda bread corned beef beer hig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description  \\\n",
       "0  Resting is part of training. I've confirmed wh...   \n",
       "1  Think of talking to yourself as a tool to coac...   \n",
       "2  The clock is ticking for the United States to ...   \n",
       "3  If you want to be busy, keep trying to be perf...   \n",
       "4  First, the bad news: Soda bread, corned beef a...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  resting part training ive confirmed sort alrea...  \n",
       "1  think talking tool coach challenge narrate exp...  \n",
       "2  clock ticking united state find cure team work...  \n",
       "3  want busy keep trying perfect want happy focus...  \n",
       "4  first bad news soda bread corned beef beer hig...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_news['clean_text'] = df_news['short_description'].apply(preprocess_text)\n",
    "df_news[['short_description', 'clean_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e413973",
   "metadata": {},
   "source": [
    "### Task 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e85f73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "241f382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55dc4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_news['clean_text']).toarray()\n",
    "y = df_news['category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b30d5229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (50000, 5000)\n",
      "Number of classes: 10\n",
      "Sample classes: ['WELLNESS' 'POLITICS' 'ENTERTAINMENT' 'TRAVEL' 'STYLE & BEAUTY'\n",
      " 'PARENTING' 'FOOD & DRINK' 'WORLD NEWS' 'BUSINESS' 'SPORTS']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Number of classes:\", y.nunique())\n",
    "print(\"Sample classes:\", y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3a112",
   "metadata": {},
   "source": [
    "### Task 3: Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdebbf5",
   "metadata": {},
   "source": [
    "#### Step 1: Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71538ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6f10c",
   "metadata": {},
   "source": [
    "#### Step 2: Train Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c1b9a",
   "metadata": {},
   "source": [
    "#####  1. Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c5301b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96eaed",
   "metadata": {},
   "source": [
    "##### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "955a38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa1367",
   "metadata": {},
   "source": [
    "##### 3. Support Vector Machine (SVM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0713d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_model = LinearSVC(dual=False)\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57900ebd",
   "metadata": {},
   "source": [
    "### Task 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07b177d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a98c9",
   "metadata": {},
   "source": [
    "#####  1. Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a2053d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      BUSINESS       0.63      0.68      0.65       955\n",
      " ENTERTAINMENT       0.55      0.56      0.55       985\n",
      "  FOOD & DRINK       0.69      0.70      0.70      1021\n",
      "     PARENTING       0.67      0.63      0.65      1030\n",
      "      POLITICS       0.66      0.60      0.63      1034\n",
      "        SPORTS       0.66      0.72      0.69       995\n",
      "STYLE & BEAUTY       0.73      0.70      0.71       986\n",
      "        TRAVEL       0.72      0.67      0.69      1008\n",
      "      WELLNESS       0.63      0.67      0.65      1009\n",
      "    WORLD NEWS       0.66      0.66      0.66       977\n",
      "\n",
      "      accuracy                           0.66     10000\n",
      "     macro avg       0.66      0.66      0.66     10000\n",
      "  weighted avg       0.66      0.66      0.66     10000\n",
      "\n",
      "Accuracy: 0.6581\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f12576",
   "metadata": {},
   "source": [
    "##### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5647708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      BUSINESS       0.58      0.64      0.61       955\n",
      " ENTERTAINMENT       0.63      0.51      0.56       985\n",
      "  FOOD & DRINK       0.69      0.72      0.70      1021\n",
      "     PARENTING       0.53      0.63      0.57      1030\n",
      "      POLITICS       0.67      0.58      0.62      1034\n",
      "        SPORTS       0.72      0.66      0.68       995\n",
      "STYLE & BEAUTY       0.69      0.69      0.69       986\n",
      "        TRAVEL       0.68      0.66      0.67      1008\n",
      "      WELLNESS       0.59      0.65      0.62      1009\n",
      "    WORLD NEWS       0.67      0.69      0.68       977\n",
      "\n",
      "      accuracy                           0.64     10000\n",
      "     macro avg       0.65      0.64      0.64     10000\n",
      "  weighted avg       0.65      0.64      0.64     10000\n",
      "\n",
      "Accuracy: 0.641\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Performance:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7fd0f",
   "metadata": {},
   "source": [
    "##### 3. Support Vector Machine (SVM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3159e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Performance:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      BUSINESS       0.65      0.72      0.68       955\n",
      " ENTERTAINMENT       0.54      0.53      0.53       985\n",
      "  FOOD & DRINK       0.69      0.70      0.69      1021\n",
      "     PARENTING       0.64      0.61      0.62      1030\n",
      "      POLITICS       0.64      0.59      0.61      1034\n",
      "        SPORTS       0.67      0.75      0.71       995\n",
      "STYLE & BEAUTY       0.71      0.70      0.71       986\n",
      "        TRAVEL       0.68      0.64      0.66      1008\n",
      "      WELLNESS       0.63      0.65      0.64      1009\n",
      "    WORLD NEWS       0.65      0.65      0.65       977\n",
      "\n",
      "      accuracy                           0.65     10000\n",
      "     macro avg       0.65      0.65      0.65     10000\n",
      "  weighted avg       0.65      0.65      0.65     10000\n",
      "\n",
      "Accuracy: 0.6517\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Performance:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12f31e",
   "metadata": {},
   "source": [
    "#####  Comparing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "006aada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.6581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.6410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.6517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy\n",
       "Logistic Regression    0.6581\n",
       "Naive Bayes            0.6410\n",
       "SVM                    0.6517"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = {\n",
    "    \"Logistic Regression\": accuracy_score(y_test, y_pred_lr),\n",
    "    \"Naive Bayes\": accuracy_score(y_test, y_pred_nb),\n",
    "    \"SVM\": accuracy_score(y_test, y_pred_svm)\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(model_scores, orient='index', columns=['Accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0d623",
   "metadata": {},
   "source": [
    "### Final Summary â€“ News Article Classification\n",
    "\n",
    "#### In Part B of the project, I worked on classifying news articles into categories like politics, sports, and technology. The dataset included short descriptions of news articles along with their respective categories.\n",
    "\n",
    "##### To prepare the data, I first cleaned the text by converting it to lowercase, removing punctuation, numbers, and stopwords, and applying lemmatization to simplify the words. Then, I used TF-IDF vectorization to convert the cleaned text into numerical features that could be understood by machine learning models.\n",
    "\n",
    "#### I trained three different models: Logistic Regression, Naive Bayes, and Support Vector Machine (SVM). After splitting the data into training and testing sets, I evaluated each model using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "#### All models gave reasonable results, but Logistic Regression and SVM performed the best, giving higher accuracy and balanced classification across all categories. The Naive Bayes model was slightly faster but a bit less accurate.\n",
    "\n",
    "#### This project helped me apply real-world NLP techniques to a text classification problem and gave me a better understanding of how to build and evaluate models that work with textual data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
